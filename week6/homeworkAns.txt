一、

ERINE1.0
1、更多的数据.
2、实体级别的MASK.
3、通过多轮对话修改NSP任务.

XLNET
1、融合了ar与ae模型的优点、引入PLM,不用真正打乱原始语序.
2、启用了双流注意力,把语音模型的优势引入进来,只关注当前预测的位置编码,不关注文本信息.
3、只预测整段文本的后半部分,减少计算量.
4、引入TransforerXl中的循环结构与相对位置编码.

Robert:
1、更长的训练时间、更大bacth size、更多的训练数据.
2、去掉了NSP任务.
3、启用了动态mask(实际上是生成更多的数据).
4、字节级别的编码,能获得更加细粒度的token.

Albert:
1、通过矩阵分解的办法,分离了E的大小与H的大小.使得H的大小可以更大并且不影响emb的维度大小.
2、参数共享.使得训练的参数更少,训练更快,而且模型的预测效果更佳的稳定.
3、SOP任务替换了NSP任务.

二、
原版的bert的NSP,事实上是不同领域的两个句子构造的NSP负例.
所以原版的NSP任务变成了预测:1、两个句子是否在同一领域.2、两个句子是否通顺.这两个情况.
很有可能模型学习到的是1而不是2.导致模型精度下降.

启用了SOP可以分离1、2这两种情况.
明确了负例只能是那些不通顺的句子.